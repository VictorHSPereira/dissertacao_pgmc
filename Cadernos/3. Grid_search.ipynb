{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports necessários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from permetrics.regression import RegressionMetric\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import statistics as st\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(1, '..\\Modelo')\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lssvr import LSSVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from eTS import eTS\n",
    "from Simpl_eTS import Simpl_eTS\n",
    "from exTS import exTS\n",
    "from ePL import ePL\n",
    "from eMG import eMG\n",
    "from ePL_plus import ePL_plus\n",
    "from ePL_KRLS_DISCO import ePL_KRLS_DISCO\n",
    "from NMFIS import NMFIS\n",
    "from NTSK import NTSK\n",
    "from GEN_NMFIS import GEN_NMFIS\n",
    "from GEN_NTSK import GEN_NTSK\n",
    "\n",
    "# Neural Network\n",
    "# Model\n",
    "from keras.models import Sequential, Model\n",
    "# Layers\n",
    "from keras.layers import Input, InputLayer, Dense, Dropout, Conv1D, GRU, LSTM, MaxPooling1D, Flatten, SimpleRNN\n",
    "#from tensorflow.keras.layers import Input\n",
    "from tcn import TCN\n",
    "# Optimizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "# Save network\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando a série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mes</th>\n",
       "      <th>hora_sin</th>\n",
       "      <th>hora_cos</th>\n",
       "      <th>dia_semana_sin</th>\n",
       "      <th>dia_semana_cos</th>\n",
       "      <th>QTD_VEICULOS_USINA</th>\n",
       "      <th>primeira_quinzena</th>\n",
       "      <th>feriado</th>\n",
       "      <th>fim_semana</th>\n",
       "      <th>lag_1h</th>\n",
       "      <th>lag_1d</th>\n",
       "      <th>lag_7d</th>\n",
       "      <th>TPV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.659258e-01</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.840000</td>\n",
       "      <td>6.538125</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>8.484063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.885333</td>\n",
       "      <td>6.293235</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>8.150294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.484063</td>\n",
       "      <td>6.093056</td>\n",
       "      <td>2.476667</td>\n",
       "      <td>7.830648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.150294</td>\n",
       "      <td>5.945197</td>\n",
       "      <td>2.605000</td>\n",
       "      <td>7.581667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.830648</td>\n",
       "      <td>5.780604</td>\n",
       "      <td>2.682000</td>\n",
       "      <td>7.362583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.581667</td>\n",
       "      <td>5.620099</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>7.186905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.362583</td>\n",
       "      <td>5.523352</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>6.978712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.186905</td>\n",
       "      <td>5.412337</td>\n",
       "      <td>2.797500</td>\n",
       "      <td>6.898043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.978712</td>\n",
       "      <td>5.309656</td>\n",
       "      <td>2.818889</td>\n",
       "      <td>6.772153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.898043</td>\n",
       "      <td>5.209537</td>\n",
       "      <td>2.808000</td>\n",
       "      <td>6.632307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mes      hora_sin  hora_cos  dia_semana_sin  dia_semana_cos  \\\n",
       "0    1  9.659258e-01 -0.258819       -0.974928       -0.222521   \n",
       "1    1  8.660254e-01 -0.500000       -0.974928       -0.222521   \n",
       "2    1  8.660254e-01 -0.500000       -0.974928       -0.222521   \n",
       "3    1  7.071068e-01 -0.707107       -0.974928       -0.222521   \n",
       "4    1  7.071068e-01 -0.707107       -0.974928       -0.222521   \n",
       "5    1  5.000000e-01 -0.866025       -0.974928       -0.222521   \n",
       "6    1  5.000000e-01 -0.866025       -0.974928       -0.222521   \n",
       "7    1  2.588190e-01 -0.965926       -0.974928       -0.222521   \n",
       "8    1  2.588190e-01 -0.965926       -0.974928       -0.222521   \n",
       "9    1  1.224647e-16 -1.000000       -0.974928       -0.222521   \n",
       "\n",
       "   QTD_VEICULOS_USINA  primeira_quinzena  feriado  fim_semana    lag_1h  \\\n",
       "0                  21                  1        0           1  8.840000   \n",
       "1                  23                  1        0           1  8.885333   \n",
       "2                  28                  1        0           1  8.484063   \n",
       "3                  35                  1        0           1  8.150294   \n",
       "4                  32                  1        0           1  7.830648   \n",
       "5                  29                  1        0           1  7.581667   \n",
       "6                  29                  1        0           1  7.362583   \n",
       "7                  30                  1        0           1  7.186905   \n",
       "8                  32                  1        0           1  6.978712   \n",
       "9                  31                  1        0           1  6.898043   \n",
       "\n",
       "     lag_1d    lag_7d       TPV  \n",
       "0  6.538125  2.220000  8.484063  \n",
       "1  6.293235  2.220000  8.150294  \n",
       "2  6.093056  2.476667  7.830648  \n",
       "3  5.945197  2.605000  7.581667  \n",
       "4  5.780604  2.682000  7.362583  \n",
       "5  5.620099  2.733333  7.186905  \n",
       "6  5.523352  2.770000  6.978712  \n",
       "7  5.412337  2.797500  6.898043  \n",
       "8  5.309656  2.818889  6.772153  \n",
       "9  5.209537  2.808000  6.632307  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Dados/Dados refinados/timeseries.csv\", index_col= 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando X e y\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo o scalers\n",
    "scaler_X, scaler_y = MinMaxScaler(),MinMaxScaler()\n",
    "\n",
    "#fit\n",
    "scaler_X.fit(X[['mes','QTD_VEICULOS_USINA']])\n",
    "scaler_y.fit(pd.DataFrame(y))\n",
    "\n",
    "#transform\n",
    "\n",
    "X[['mes','QTD_VEICULOS_USINA']] = scaler_X.transform(X[['mes','QTD_VEICULOS_USINA']])\n",
    "y = scaler_y.transform(pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['mes', 'primeira_quinzena', 'feriado', 'fim_semana']\n",
    "\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10271"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os índices do treino, validacao e teste\n",
    "\n",
    "index_train = round(X.shape[0]*0.5)\n",
    "index_val = round(X.shape[0]*0.3) + index_train\n",
    "\n",
    "#Separando as bases\n",
    "X_train, y_train = X.values[:index_train], y[:index_train]\n",
    "X_val, y_val = X.values[index_train:index_val], y[index_train:index_val]\n",
    "X_test, y_test =  X.values[index_val:], y[index_val:]\n",
    "\n",
    "\n",
    "y_test = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Clássicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "columns = [\"Model_Name\", \"NRMSE\", \"NDEI\", \"MAPE\", \"Rules\", \"Best_Params\"]\n",
    "results = pd.DataFrame(columns = columns)\n",
    "\n",
    "# Predictions\n",
    "predictions = pd.DataFrame(y_test, columns= ['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] {'n_neighbors': 2}\n",
      "[2/6] {'n_neighbors': 3}\n",
      "[3/6] {'n_neighbors': 4}\n",
      "[4/6] {'n_neighbors': 5}\n",
      "[5/6] {'n_neighbors': 10}\n",
      "[6/6] {'n_neighbors': 20}\n",
      "\n",
      "Results for KNN: \n",
      "\n",
      "\n",
      "RMSE: 1.5303760205592174\n",
      "NRMSE: 0.5508163043897\n",
      "NDEI: 0.5430557275858096\n",
      "MAE: 0.8654752944122002\n",
      "MAPE: 0.44578320258869514\n",
      "Rules: -\n",
      "\n",
      "Best params: \n",
      "{'n_neighbors': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thevh\\AppData\\Local\\Temp\\ipykernel_1508\\92911531.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, newrow], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"KNN\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'n_neighbors': [2, 3, 4, 5, 10, 20]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "  \n",
    "\n",
    "    # Optimize parameters\n",
    "    KNN = KNeighborsRegressor(**param)\n",
    "    KNN.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = KNN.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_KNN_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "\n",
    "KNN = KNeighborsRegressor(**best_KNN_params)\n",
    "KNN.fit(new_X_train, new_y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = KNN.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "\n",
    "print(f'\\nBest params: \\n{best_KNN_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_KNN_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9] {'max_depth': 2}\n",
      "[2/9] {'max_depth': 4}\n",
      "[3/9] {'max_depth': 6}\n",
      "[4/9] {'max_depth': 8}\n",
      "[5/9] {'max_depth': 10}\n",
      "[6/9] {'max_depth': 20}\n",
      "[7/9] {'max_depth': 30}\n",
      "[8/9] {'max_depth': 50}\n",
      "[9/9] {'max_depth': 100}\n",
      "\n",
      "Results for Regression Tree: \n",
      "\n",
      "\n",
      "RMSE: 0.8452041879227032\n",
      "NRMSE: 0.2926542016926031\n",
      "NDEI: 0.2999216983700617\n",
      "MAE: 0.3504340154219335\n",
      "MAPE: 0.11005237502562898\n",
      "Rules: -\n",
      "\n",
      "Best params: \n",
      "{'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"Regression Tree\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'max_depth': [2, 4, 6, 8, 10, 20, 30, 50, 100]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    RT = DecisionTreeRegressor(**param)\n",
    "    RT.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = RT.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_RT_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "\n",
    "RT = DecisionTreeRegressor(**best_RT_params)\n",
    "RT.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = RT.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "print(f'\\nBest params: \\n{best_RT_params}')\n",
    "   \n",
    "RT_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_RT_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] {'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "[2/60] {'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "[3/60] {'C': 0.01, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "[4/60] {'C': 0.01, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "[5/60] {'C': 0.01, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "[6/60] {'C': 0.01, 'gamma': 0.5, 'kernel': 'sigmoid'}\n",
      "[7/60] {'C': 0.01, 'gamma': 1, 'kernel': 'linear'}\n",
      "[8/60] {'C': 0.01, 'gamma': 1, 'kernel': 'rbf'}\n",
      "[9/60] {'C': 0.01, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "[10/60] {'C': 0.01, 'gamma': 10, 'kernel': 'linear'}\n",
      "[11/60] {'C': 0.01, 'gamma': 10, 'kernel': 'rbf'}\n",
      "[12/60] {'C': 0.01, 'gamma': 10, 'kernel': 'sigmoid'}\n",
      "[13/60] {'C': 0.01, 'gamma': 50, 'kernel': 'linear'}\n",
      "[14/60] {'C': 0.01, 'gamma': 50, 'kernel': 'rbf'}\n",
      "[15/60] {'C': 0.01, 'gamma': 50, 'kernel': 'sigmoid'}\n",
      "[16/60] {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "[17/60] {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "[18/60] {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "[19/60] {'C': 0.1, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "[20/60] {'C': 0.1, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "[21/60] {'C': 0.1, 'gamma': 0.5, 'kernel': 'sigmoid'}\n",
      "[22/60] {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n",
      "[23/60] {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "[24/60] {'C': 0.1, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "[25/60] {'C': 0.1, 'gamma': 10, 'kernel': 'linear'}\n",
      "[26/60] {'C': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "[27/60] {'C': 0.1, 'gamma': 10, 'kernel': 'sigmoid'}\n",
      "[28/60] {'C': 0.1, 'gamma': 50, 'kernel': 'linear'}\n",
      "[29/60] {'C': 0.1, 'gamma': 50, 'kernel': 'rbf'}\n",
      "[30/60] {'C': 0.1, 'gamma': 50, 'kernel': 'sigmoid'}\n",
      "[31/60] {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "[32/60] {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "[33/60] {'C': 1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "[34/60] {'C': 1, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "[35/60] {'C': 1, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "[36/60] {'C': 1, 'gamma': 0.5, 'kernel': 'sigmoid'}\n",
      "[37/60] {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "[38/60] {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "[39/60] {'C': 1, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "[40/60] {'C': 1, 'gamma': 10, 'kernel': 'linear'}\n",
      "[41/60] {'C': 1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "[42/60] {'C': 1, 'gamma': 10, 'kernel': 'sigmoid'}\n",
      "[43/60] {'C': 1, 'gamma': 50, 'kernel': 'linear'}\n",
      "[44/60] {'C': 1, 'gamma': 50, 'kernel': 'rbf'}\n",
      "[45/60] {'C': 1, 'gamma': 50, 'kernel': 'sigmoid'}\n",
      "[46/60] {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "[47/60] {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "[48/60] {'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "[49/60] {'C': 10, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "[50/60] {'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "[51/60] {'C': 10, 'gamma': 0.5, 'kernel': 'sigmoid'}\n",
      "[52/60] {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
      "[53/60] {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "[54/60] {'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "[55/60] {'C': 10, 'gamma': 10, 'kernel': 'linear'}\n",
      "[56/60] {'C': 10, 'gamma': 10, 'kernel': 'rbf'}\n",
      "[57/60] {'C': 10, 'gamma': 10, 'kernel': 'sigmoid'}\n",
      "[58/60] {'C': 10, 'gamma': 50, 'kernel': 'linear'}\n",
      "[59/60] {'C': 10, 'gamma': 50, 'kernel': 'rbf'}\n",
      "[60/60] {'C': 10, 'gamma': 50, 'kernel': 'sigmoid'}\n",
      "\n",
      "Results for SVM: \n",
      "\n",
      "\n",
      "RMSE: 3.3735581147603146\n",
      "NRMSE: 1.2758326755185818\n",
      "NDEI: 1.1971110576436825\n",
      "MAE: 3.2877162890250764\n",
      "MAPE: 2.1690629888253965\n",
      "Rules: -\n",
      "\n",
      "{'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"SVM\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'kernel': ['linear', 'rbf', 'sigmoid'], 'C':[0.01,0.1,1,10], 'gamma': [0.01,0.5,1,10,50]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "    \n",
    "    # Optimize parameters\n",
    "    SVM = SVR(**param)\n",
    "    SVM.fit(X_train,y_train.ravel())\n",
    "    # Make predictions\n",
    "    y_pred = SVM.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_SVM_params = param\n",
    "\n",
    "# # Use best parameters1 to save time looking for best parameters2\n",
    "# parameters2 = {'kernel': ['poly'], 'C':[best_SVM_params['C']], 'gamma': [best_SVM_params['gamma']], 'degree': [2,3]}\n",
    "\n",
    "# grid2 = ParameterGrid(parameters2)\n",
    "\n",
    "# for param in grid2:\n",
    "    \n",
    "#     #print(param)\n",
    "    \n",
    "#     # Optimize parameters\n",
    "#     SVM = SVR(**param)\n",
    "#     SVM.fit(X_train,y_train)\n",
    "#     # Make predictions\n",
    "#     y_pred = SVM.predict(X_val)\n",
    "#     \n",
    "    \n",
    "#     # Calculating the error metrics\n",
    "#     # Compute the Root Mean Square Error\n",
    "#     RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "#     if RMSE < lower_rmse:\n",
    "#         lower_rmse = RMSE\n",
    "#         best_SVM_params = param\n",
    "\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "\n",
    "SVM = SVR(**best_SVM_params)\n",
    "SVM.fit(new_X_train,new_y_train.ravel())\n",
    "# Make predictions\n",
    "y_pred = SVM.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "SVM_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "print(f'\\n{best_SVM_params}')\n",
    "   \n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_SVM_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/40] {'learning_rate': 0.01, 'n_estimators': 2}\n",
      "[2/40] {'learning_rate': 0.01, 'n_estimators': 4}\n",
      "[3/40] {'learning_rate': 0.01, 'n_estimators': 8}\n",
      "[4/40] {'learning_rate': 0.01, 'n_estimators': 16}\n",
      "[5/40] {'learning_rate': 0.01, 'n_estimators': 32}\n",
      "[6/40] {'learning_rate': 0.01, 'n_estimators': 64}\n",
      "[7/40] {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "[8/40] {'learning_rate': 0.01, 'n_estimators': 200}\n",
      "[9/40] {'learning_rate': 0.05, 'n_estimators': 2}\n",
      "[10/40] {'learning_rate': 0.05, 'n_estimators': 4}\n",
      "[11/40] {'learning_rate': 0.05, 'n_estimators': 8}\n",
      "[12/40] {'learning_rate': 0.05, 'n_estimators': 16}\n",
      "[13/40] {'learning_rate': 0.05, 'n_estimators': 32}\n",
      "[14/40] {'learning_rate': 0.05, 'n_estimators': 64}\n",
      "[15/40] {'learning_rate': 0.05, 'n_estimators': 100}\n",
      "[16/40] {'learning_rate': 0.05, 'n_estimators': 200}\n",
      "[17/40] {'learning_rate': 0.1, 'n_estimators': 2}\n",
      "[18/40] {'learning_rate': 0.1, 'n_estimators': 4}\n",
      "[19/40] {'learning_rate': 0.1, 'n_estimators': 8}\n",
      "[20/40] {'learning_rate': 0.1, 'n_estimators': 16}\n",
      "[21/40] {'learning_rate': 0.1, 'n_estimators': 32}\n",
      "[22/40] {'learning_rate': 0.1, 'n_estimators': 64}\n",
      "[23/40] {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "[24/40] {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "[25/40] {'learning_rate': 0.5, 'n_estimators': 2}\n",
      "[26/40] {'learning_rate': 0.5, 'n_estimators': 4}\n",
      "[27/40] {'learning_rate': 0.5, 'n_estimators': 8}\n",
      "[28/40] {'learning_rate': 0.5, 'n_estimators': 16}\n",
      "[29/40] {'learning_rate': 0.5, 'n_estimators': 32}\n",
      "[30/40] {'learning_rate': 0.5, 'n_estimators': 64}\n",
      "[31/40] {'learning_rate': 0.5, 'n_estimators': 100}\n",
      "[32/40] {'learning_rate': 0.5, 'n_estimators': 200}\n",
      "[33/40] {'learning_rate': 0.9, 'n_estimators': 2}\n",
      "[34/40] {'learning_rate': 0.9, 'n_estimators': 4}\n",
      "[35/40] {'learning_rate': 0.9, 'n_estimators': 8}\n",
      "[36/40] {'learning_rate': 0.9, 'n_estimators': 16}\n",
      "[37/40] {'learning_rate': 0.9, 'n_estimators': 32}\n",
      "[38/40] {'learning_rate': 0.9, 'n_estimators': 64}\n",
      "[39/40] {'learning_rate': 0.9, 'n_estimators': 100}\n",
      "[40/40] {'learning_rate': 0.9, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thevh\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for GB: \n",
      "\n",
      "\n",
      "RMSE: 1.0001489078029921\n",
      "NRMSE: 0.41585961612248584\n",
      "NDEI: 0.35490401412761174\n",
      "MAE: 0.6997245278552686\n",
      "MAPE: 0.6655793502657171\n",
      "Rules: -\n",
      "\n",
      "{'learning_rate': 0.1, 'n_estimators': 64}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"GB\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 0.9], 'n_estimators': [2, 4, 8, 16, 32, 64, 100, 200]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    GB = GradientBoostingRegressor(**param)\n",
    "    GB.fit(X_train,y_train.ravel())\n",
    "    # Make predictions\n",
    "    y_pred = GB.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_GB_params = param\n",
    "\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "GB = GradientBoostingRegressor(**best_GB_params)\n",
    "GB.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = GB.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "GB_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "print(f'\\n{best_GB_params}')\n",
    "   \n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_GB_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[2/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[3/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[4/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[5/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[6/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[7/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[8/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[9/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[10/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[11/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[12/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[13/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[14/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[15/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[16/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[17/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[18/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[19/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[20/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[21/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[22/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[23/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[24/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[25/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[26/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[27/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[28/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[29/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[30/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[31/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[32/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[33/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[34/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[35/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[36/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[37/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[38/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[39/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[40/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[41/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[42/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[43/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[44/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[45/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[46/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[47/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[48/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[49/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[50/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[51/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[52/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[53/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[54/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[55/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[56/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[57/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[58/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[59/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[60/180] {'eta': 0.3, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[61/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[62/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[63/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[64/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[65/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[66/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[67/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[68/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[69/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[70/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[71/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[72/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[73/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[74/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[75/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[76/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[77/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[78/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[79/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[80/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[81/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[82/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[83/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[84/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[85/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[86/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[87/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[88/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[89/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[90/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[91/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[92/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[93/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[94/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[95/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[96/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[97/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[98/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[99/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[100/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[101/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[102/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[103/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[104/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[105/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[106/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[107/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[108/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[109/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[110/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[111/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[112/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[113/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[114/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[115/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[116/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[117/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[118/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[119/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[120/180] {'eta': 0.4, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[121/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[122/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[123/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[124/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[125/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[126/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[127/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[128/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[129/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[130/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[131/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[132/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[133/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[134/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[135/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[136/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[137/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[138/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[139/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[140/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.3, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[141/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[142/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[143/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[144/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[145/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[146/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[147/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[148/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[149/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[150/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[151/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[152/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[153/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[154/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[155/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[156/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[157/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[158/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[159/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[160/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[161/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[162/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[163/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[164/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[165/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[166/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[167/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[168/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[169/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[170/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[171/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[172/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[173/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[174/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[175/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[176/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "[177/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 250}\n",
      "[178/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500}\n",
      "[179/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 250}\n",
      "[180/180] {'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500}\n",
      "\n",
      "Results for XGBoost: \n",
      "\n",
      "\n",
      "RMSE: 1.0526145546630552\n",
      "NRMSE: 0.46612970353459826\n",
      "NDEI: 0.37352151051156607\n",
      "MAE: 0.7767725016899744\n",
      "MAPE: 0.8567335170743875\n",
      "Rules: -\n",
      "\n",
      "{'eta': 0.5, 'eval_metric': 'rmse', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"XGBoost\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'n_estimators':[250, 500], 'min_child_weight':[2,5], \n",
    "              'gamma':[i/10.0 for i in range(3,6)], 'max_depth': [2, 3, 4, 6, 7], \n",
    "              'eval_metric':['rmse'],'eta':[i/10.0 for i in range(3,6)]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "\n",
    "    # Optimize parameters\n",
    "    XGBoost = XGBRegressor(**param)\n",
    "    XGBoost.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = XGBoost.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_XGBoost_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "XGBoost = XGBRegressor(**best_XGBoost_params)\n",
    "XGBoost.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = XGBoost.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "   \n",
    "XGBoost_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "print(f'\\n{best_XGBoost_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_XGBoost_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] {'learning_rate': 0.01, 'verbosity': -1}\n",
      "[2/5] {'learning_rate': 0.05, 'verbosity': -1}\n",
      "[3/5] {'learning_rate': 0.1, 'verbosity': -1}\n",
      "[4/5] {'learning_rate': 0.5, 'verbosity': -1}\n",
      "[5/5] {'learning_rate': 0.9, 'verbosity': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thevh\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for LGBM: \n",
      "\n",
      "\n",
      "RMSE: 0.5974629570775071\n",
      "NRMSE: 0.22886810975381366\n",
      "NDEI: 0.21201043175175668\n",
      "MAE: 0.3819869035403932\n",
      "MAPE: 0.31449419321918587\n",
      "Rules: -\n",
      "\n",
      "{'learning_rate': 0.05, 'verbosity': -1}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"LGBM\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 0.9],'verbosity':[-1]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "    \n",
    "    # Optimize parameters\n",
    "    LGBM = LGBMRegressor(**param)\n",
    "    LGBM.fit(X_train,y_train.ravel())\n",
    "    # Make predictions\n",
    "    y_pred = LGBM.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_LGBM_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "LGBM = LGBMRegressor(**best_LGBM_params)\n",
    "LGBM.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = LGBM.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "   \n",
    "LGBM_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "print(f'\\n{best_LGBM_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_LGBM_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_DL = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val_DL = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test_DL = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thevh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert '12' to a shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [202]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_MLP_parameters)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     25\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_DL, y_train, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val_DL, y_val), callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_cb, early_stopping_cb])\n",
      "Input \u001b[1;32mIn [202]\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(n_hidden, n_neurons, activation, learning_rate, input_shape)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(n_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-3\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 6\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_hidden):\n\u001b[0;32m      8\u001b[0m         model\u001b[38;5;241m.\u001b[39madd(Dense(n_neurons, activation\u001b[38;5;241m=\u001b[39mactivation))\n",
      "File \u001b[1;32mc:\\Users\\thevh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:47\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(batch_shape)\n",
      "File \u001b[1;32mc:\\Users\\thevh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\variables.py:515\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined shapes are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, tf\u001b[38;5;241m.\u001b[39mTensorShape):\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;66;03m# `tf.TensorShape` may contain `Dimension` objects.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;66;03m# We need to convert the items in it to either int or `None`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert '12' to a shape."
     ]
    }
   ],
   "source": [
    "Model_Name = \"MLP\"\n",
    "\n",
    "# Define the function to create models for the optimization method\n",
    "def build_model(n_hidden=1, n_neurons=30, activation = \"relu\", learning_rate=3e-3, input_shape=[4]):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Set the parameters for the network\n",
    "best_MLP_parameters = {'n_hidden':2, 'n_neurons':77, 'activation':\"relu\", 'learning_rate': 0.145, 'input_shape': X_train_DL.shape[1]}\n",
    "\n",
    "# Checkpoint for early stopping\n",
    "checkpoint_cb = ModelCheckpoint(f'RandomSearchResults/{Model_Name}.keras', save_best_only=True)\n",
    "early_stopping_cb = EarlyStopping(patience=20,restore_best_weights=True)\n",
    "\n",
    "# Define the model\n",
    "model = build_model(**best_MLP_parameters)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_DL, y_train, epochs = 1000, validation_data=(X_val_DL, y_val), callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_DL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred.ravel()\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "Rules = \"-\"\n",
    "print(\"Rules:\", Rules)\n",
    "   \n",
    "MLP_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_MLP_parameters]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evolving Fuzzy Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] {'InitialOmega': 50, 'r': 0.1}\n",
      "\n",
      "Results for eTS: \n",
      "\n",
      "\n",
      "RMSE: 0.6453815145937141\n",
      "NRMSE: 0.2336932825308349\n",
      "NDEI: 0.2290143881436749\n",
      "MAE: 0.3601403476494569\n",
      "MAPE: 0.1911948655404788\n",
      "Rules: 4\n",
      "\n",
      "{'InitialOmega': 50, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"eTS\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "# parameters = {'InitialOmega': [50, 100, 250, 500, 750, 1000, 10000], 'r': [0.1, 0.3, 0.5, 0.7, 0.9, 1., 5, 10, 50]}\n",
    "\n",
    "parameters = {'InitialOmega': [50], 'r': [0.1]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = eTS(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_eTS_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = eTS(**best_eTS_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "\n",
    "   \n",
    "eTS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_eTS_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_eTS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpl_eTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] {'InitialOmega': 50, 'r': 0.1}\n",
      "\n",
      "Results for Simpl_eTS: \n",
      "\n",
      "\n",
      "RMSE: 0.8876415158524629\n",
      "NRMSE: 0.3224989795764989\n",
      "NDEI: 0.31498063400816195\n",
      "MAE: 0.5204420813679302\n",
      "MAPE: 0.2665403667925537\n",
      "Rules: 34\n",
      "\n",
      "{'InitialOmega': 50, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"Simpl_eTS\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "# parameters = {'InitialOmega': [50, 250, 500, 750, 1000], 'r': [0.1, 0.3, 0.5, 0.7]}\n",
    "parameters = {'InitialOmega': [50], 'r': [0.1]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = Simpl_eTS(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_Simpl_eTS_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = Simpl_eTS(**best_Simpl_eTS_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "Simpl_eTS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_Simpl_eTS_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_Simpl_eTS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"exTS\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'InitialOmega': [50, 250, 500, 750, 1000], 'mu_threshold': [0.1, 0.3, 0.5, 0.7]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = exTS(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_exTS_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = exTS(**best_exTS_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "exTS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_exTS_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_exTS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ePL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"ePL\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'alpha': [0.001, 0.01, 0.1, 0.5, 0.9], 'beta': [0.001, 0.005, 0.01, 0.1, 0.2], 'lambda1': [0.001, 0.01, 0.1], 's': [100, 10000], 'r': [0.1, 0.25, 0.5, 0.75]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = ePL(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_ePL_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = ePL(**best_ePL_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "ePL_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_ePL_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_ePL_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"eMG\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'alpha': [0.001, 0.01], 'lambda1': [0.1, 0.5], 'w': [10, 50], 'sigma': [0.001, 0.003], 'omega': [10**4]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = eMG(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_eMG_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = eMG(**best_eMG_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "eMG_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_eMG_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_eMG_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ePL+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"ePL_plus\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'alpha': [0.001, 0.01, 0.1], 'beta': [0.01, 0.1, 0.25], 'lambda1': [0.25, 0.5, 0.75], 'omega': [100, 10000], 'sigma': [0.1, 0.25, 0.5], 'e_Utility': [0.03, 0.05], 'pi': [0.3, 0.5]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = ePL_plus(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_ePL_plus_params = param\n",
    "\n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = ePL_plus(**best_ePL_plus_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "ePL_plus_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "print(f'\\n{best_ePL_plus_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_ePL_plus_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ePL-KRLS-DISCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"ePL_KRLS_DISCO\"\n",
    "\n",
    "# Define Grid Search parameters\n",
    "parameters = {'alpha': [0.05, 0.1], 'beta': [0.01, 0.1, 0.25], 'lambda1': [0.0000001, 0.001], 'sigma': [0.5, 1, 10, 50], 'e_utility': [0.03, 0.05]}\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = ePL_KRLS_DISCO(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_ePL_KRLS_DISCO_params = param\n",
    "        \n",
    "# Optimized parameters\n",
    "\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "model = ePL_KRLS_DISCO(**best_ePL_KRLS_DISCO_params)\n",
    "OutputTraining, Rules = model.fit(new_X_train,new_y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Number of rules\n",
    "print(\"Rules:\", Rules[-1])\n",
    "   \n",
    "ePL_KRLS_DISCO_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules[-1]}'\n",
    "\n",
    "#print(f'\\n{best_ePL_KRLS_DISCO_params}')\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules[-1], best_ePL_KRLS_DISCO_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Models (Kaike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"NMFIS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "parameters = {'n_clusters':range(1,20)}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = NMFIS(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_NMFIS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = NMFIS(**best_NMFIS_params)\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "\n",
    "# Train the model\n",
    "OutputTraining = model.fit(new_X_train, new_y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "\n",
    "\n",
    "NMFIS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_NMFIS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save  model to excel\n",
    "    model.parameters.to_excel(f'Model Summary/{Model_Name}_parameters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTSK-RLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] {'RLS_option': 1, 'lambda1': 0.95, 'n_clusters': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:102: RuntimeWarning: overflow encountered in square\n",
      "  self.ResidualTrainingPhase = np.append(self.ResidualTrainingPhase,(Output - y[k])**2)\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: overflow encountered in multiply\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: invalid value encountered in add\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:95: RuntimeWarning: invalid value encountered in matmul\n",
      "  Output =  xe.T @ self.Theta\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: invalid value encountered in matmul\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5] {'RLS_option': 1, 'lambda1': 0.96, 'n_clusters': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:102: RuntimeWarning: overflow encountered in square\n",
      "  self.ResidualTrainingPhase = np.append(self.ResidualTrainingPhase,(Output - y[k])**2)\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: overflow encountered in multiply\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:95: RuntimeWarning: overflow encountered in matmul\n",
      "  Output =  xe.T @ self.Theta\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:95: RuntimeWarning: invalid value encountered in matmul\n",
      "  Output =  xe.T @ self.Theta\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: overflow encountered in matmul\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: invalid value encountered in matmul\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5] {'RLS_option': 1, 'lambda1': 0.97, 'n_clusters': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:102: RuntimeWarning: overflow encountered in square\n",
      "  self.ResidualTrainingPhase = np.append(self.ResidualTrainingPhase,(Output - y[k])**2)\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: overflow encountered in multiply\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:95: RuntimeWarning: overflow encountered in matmul\n",
      "  Output =  xe.T @ self.Theta\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:95: RuntimeWarning: invalid value encountered in matmul\n",
      "  Output =  xe.T @ self.Theta\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: overflow encountered in matmul\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n",
      "d:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:188: RuntimeWarning: invalid value encountered in matmul\n",
      "  self.Theta = self.Theta + ( self.P @ xe ) * (y - xe.T @ self.Theta )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5] {'RLS_option': 1, 'lambda1': 0.98, 'n_clusters': 1}\n",
      "[5/5] {'RLS_option': 1, 'lambda1': 0.99, 'n_clusters': 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [219]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Optimize parameters\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m NTSK(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dissertação\\dissertacao_pgmc\\Cadernos\\..\\Modelo\\NTSK.py:101\u001b[0m, in \u001b[0;36mNTSK.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     99\u001b[0m         Output \u001b[38;5;241m=\u001b[39m  xe\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mloc[k, m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTheta\u001b[39m\u001b[38;5;124m'\u001b[39m]            \n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Store the results\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOutputTrainingPhase \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOutputTrainingPhase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mResidualTrainingPhase \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mResidualTrainingPhase,(Output \u001b[38;5;241m-\u001b[39m y[k])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOutputTrainingPhase\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5497\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5498\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Model_Name = \"NTSK-RLS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "parameters = {'n_clusters':[1], 'lambda1':[0.95,0.96,0.97,0.98,0.99], 'RLS_option':[1]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = NTSK(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_NTSK_RLS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = NTSK(**best_NTSK_RLS_params)\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "\n",
    "# Train the model\n",
    "OutputTraining = model.fit(new_X_train, new_y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)  \n",
    "\n",
    "NTSK_RLS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_NTSK_RLS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save the model to excel\n",
    "    model.parameters.to_excel(f'../Relatórios/Model Summary/{Model_Name}_parameters.xlsx')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the model parameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTSK-wRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] {'RLS_option': 2, 'n_clusters': 1}\n",
      "\n",
      "Results for NTSK-wRLS: \n",
      "\n",
      "\n",
      "RMSE: 0.6450422809739007\n",
      "NRMSE: 0.2335581743594338\n",
      "NDEI: 0.22889401069541743\n",
      "MAE: 0.3594328388108757\n",
      "MAPE: 0.1907822388548914\n",
      "Rules: 1\n",
      "An error occurred while saving the model parameters: Cannot save file into a non-existent directory: 'Model Summary'\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"NTSK-wRLS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "# parameters = {'n_clusters':range(1,20), 'RLS_option':[2]}\n",
    "parameters = {'n_clusters':range(1,2), 'RLS_option':[2]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\")\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = NTSK(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_NTSK_wRLS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = NTSK(**best_NTSK_wRLS_params)\n",
    "# Train the model\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "OutputTraining = model.fit(new_X_train, new_y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)  \n",
    "\n",
    "\n",
    "\n",
    "NTSK_wRLS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_NTSK_wRLS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save the model to excel\n",
    "    model.parameters.to_excel(f'../Relatórios/Model Summary/{Model_Name}_parameters.xlsx')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the model parameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEN-NMFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] {'n_clusters': 1}\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"GEN-NMFIS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "# parameters = {'n_clusters':range(1,20)}\n",
    "parameters = {'n_clusters':range(1,2)}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = GEN_NMFIS(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_GEN_NMFIS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = GEN_NMFIS(**best_GEN_NMFIS_params)\n",
    "# Train the model\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "OutputTraining = model.fit(new_X_train, new_y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)\n",
    "\n",
    "\n",
    "GEN_NMFIS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_GEN_NMFIS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save the model to excel\n",
    "    model.parameters.to_excel(f'../Relatórios/Model Summary/{Model_Name}_parameters.xlsx')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the model parameters: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEN-NTSK-RLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"GEN-NTSK-RLS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "# parameters = {'n_clusters':[1], 'lambda1':[0.95,0.96,0.97,0.98,0.99], 'RLS_option':[1]}\n",
    "parameters = {'n_clusters':[1], 'lambda1':[0.95], 'RLS_option':[1]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = GEN_NTSK(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_GEN_NTSK_RLS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = GEN_NTSK(**best_GEN_NTSK_RLS_params)\n",
    "# Train the model\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "OutputTraining = model.fit(new_X_train, new_y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)  \n",
    "\n",
    "GEN_NTSK_RLS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_GEN_NTSK_RLS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save the model to excel\n",
    "    model.parameters.to_excel(f'../Relatórios/Model Summary/{Model_Name}_parameters.xlsx')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the model parameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEN-NTSK-wRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"GEN-NTSK-wRLS\"\n",
    "\n",
    "# Set hyperparameters range\n",
    "# parameters = {'n_clusters':range(1,20), 'RLS_option':[2]}\n",
    "parameters = {'n_clusters':range(1,2), 'RLS_option':[2]}\n",
    "\n",
    "grid = ParameterGrid(parameters)\n",
    "\n",
    "lower_rmse = np.inf\n",
    "for i,param in enumerate(grid):\n",
    "    \n",
    "    print(f\"[{i+1}/{len(grid)}] {param}\" )\n",
    "\n",
    "    # Optimize parameters\n",
    "    model = GEN_NTSK(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    # Calculating the error metrics\n",
    "    # Compute the Root Mean Square Error\n",
    "    RMSE = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    if RMSE < lower_rmse:\n",
    "        lower_rmse = RMSE\n",
    "        best_GEN_NTSK_wRLS_params = param\n",
    "\n",
    "# Initialize the model\n",
    "model = GEN_NTSK(**best_GEN_NTSK_wRLS_params)\n",
    "# Train the model\n",
    "new_X_train, new_y_train = X.values[:index_val], y[:index_val]\n",
    "OutputTraining = model.fit(X_train, y_train)\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to dataframe\n",
    "y_pred = scaler_y.inverse_transform(y_pred.reshape(1, -1)).reshape(-1,1)\n",
    "predictions[f'{Model_Name}'] = y_pred\n",
    "\n",
    "# Model name\n",
    "print(f'\\nResults for {Model_Name}: \\n')\n",
    "\n",
    "# Calculating the error metrics\n",
    "# Compute the Root Mean Square Error\n",
    "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nRMSE:\", RMSE)\n",
    "# Compute the Normalized Root Mean Square Error\n",
    "NRMSE = RegressionMetric(y_test, y_pred).normalized_root_mean_square_error()\n",
    "print(\"NRMSE:\", NRMSE)\n",
    "# Compute the Non-Dimensional Error Index\n",
    "NDEI= RMSE/st.stdev(np.asfarray(y_test.flatten()))\n",
    "print(\"NDEI:\", NDEI)\n",
    "# Compute the Mean Absolute Error\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", MAE)\n",
    "# Compute the Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE:\", MAPE)\n",
    "# Compute the number of final rules\n",
    "Rules = model.model.parameters.shape[0]\n",
    "print(\"Rules:\", Rules)  \n",
    "\n",
    "\n",
    "GEN_NTSK_wRLS_ = f'{Model_Name} & {NRMSE:.2f} & {NDEI:.2f} & {MAPE:.2f} & {Rules}'\n",
    "\n",
    "# Store results to dataframe\n",
    "newrow = pd.DataFrame([[Model_Name, NRMSE, NDEI, MAPE, Rules, best_GEN_NTSK_wRLS_params]], columns=columns)\n",
    "results = pd.concat([results, newrow], ignore_index=True)\n",
    "\n",
    "# salvando em planilha\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "results.to_excel(f\"../Relatórios/results_{timestamp}.xlsx\")\n",
    "predictions.to_excel(f\"../Relatórios/predictions_{timestamp}.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Save the model to excel\n",
    "    model.parameters.to_excel(f'../Relatórios/Model Summary/{Model_Name}_parameters.xlsx')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the model parameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
